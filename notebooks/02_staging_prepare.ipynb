{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6715d869-49ec-4d75-9331-5193cd74a0e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# STAGING PREPARE (LAST SNAPSHOT ONLY)\n",
    "# - read bronze\n",
    "# - derive snapshot_date\n",
    "# - keep ONLY latest snapshot\n",
    "# - dedupe\n",
    "# - compute row_hash\n",
    "# - write clean staging table\n",
    "# =========================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ---- TABLES ----\n",
    "bronze_table = \"etl_demo.bronze.customer_bronze\"\n",
    "stg_table    = \"etl_demo.silver_staging.customer_snapshot_stg\"\n",
    "\n",
    "# ---- 1) READ BRONZE ----\n",
    "df = spark.table(bronze_table)\n",
    "\n",
    "# ---- 2) DERIVE snapshot_date ----\n",
    "# If CSV already has snapshot_date column -> use it\n",
    "if \"snapshot_date\" in df.columns:\n",
    "    df2 = df.withColumn(\"snapshot_date\", F.to_date(F.col(\"snapshot_date\")))\n",
    "else:\n",
    "    # Extract from file name: snapshot_YYYY_MM_DD.csv\n",
    "    df2 = (\n",
    "        df.withColumn(\n",
    "            \"snapshot_date_str\",\n",
    "            F.regexp_extract(\n",
    "                F.col(\"_file_path\"),\n",
    "                r\"snapshot_(\\d{4}_\\d{2}_\\d{2})\\.csv$\",\n",
    "                1\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"snapshot_date\",\n",
    "            F.to_date(F.regexp_replace(\"snapshot_date_str\", \"_\", \"-\"))\n",
    "        )\n",
    "        .drop(\"snapshot_date_str\")\n",
    "    )\n",
    "\n",
    "# ---- 3) BASIC DATA QUALITY (HARD FILTER FOR STAGING) ----\n",
    "df_valid = (\n",
    "    df2\n",
    "    .filter(F.col(\"customer_id\").isNotNull())\n",
    "    .filter(F.col(\"snapshot_date\").isNotNull())\n",
    ")\n",
    "\n",
    "# ---- 4) FIND LATEST SNAPSHOT_DATE ----\n",
    "latest_snapshot_date = (\n",
    "    df_valid\n",
    "    .agg(F.max(\"snapshot_date\").alias(\"max_snapshot_date\"))\n",
    "    .collect()[0][\"max_snapshot_date\"]\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Œ Latest snapshot_date:\", latest_snapshot_date)\n",
    "\n",
    "# ---- 5) KEEP ONLY LATEST SNAPSHOT ----\n",
    "df_latest = df_valid.filter(F.col(\"snapshot_date\") == F.lit(latest_snapshot_date))\n",
    "\n",
    "# ---- 6) DEDUPLICATION (PER CUSTOMER PER SNAPSHOT) ----\n",
    "w = (\n",
    "    Window\n",
    "    .partitionBy(\"customer_id\", \"snapshot_date\")\n",
    "    .orderBy(F.col(\"_ingest_time\").desc())\n",
    ")\n",
    "\n",
    "df_dedup = (\n",
    "    df_latest\n",
    "    .withColumn(\"_rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"_rn\") == 1)\n",
    "    .drop(\"_rn\")\n",
    ")\n",
    "\n",
    "# ---- 7) HASH FOR CHANGE DETECTION ----\n",
    "business_cols = [c for c in [\"name\", \"city\", \"email\"] if c in df_dedup.columns]\n",
    "\n",
    "df_stg = (\n",
    "    df_dedup\n",
    "    .withColumn(\n",
    "        \"row_hash\",\n",
    "        F.sha2(\n",
    "            F.concat_ws(\n",
    "                \"||\",\n",
    "                *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in business_cols]\n",
    "            ),\n",
    "            256\n",
    "        )\n",
    "    )\n",
    "    .select(\n",
    "        \"customer_id\",\n",
    "        *business_cols,\n",
    "        \"snapshot_date\",\n",
    "        \"row_hash\",\n",
    "        \"_ingest_time\",\n",
    "        \"_file_path\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---- 8) WRITE STAGING (OVERWRITE IS EXPECTED) ----\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS etl_demo.silver_staging\")\n",
    "\n",
    "(\n",
    "    df_stg.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(stg_table)\n",
    ")\n",
    "\n",
    "print(\"âœ… STAGING READY:\", stg_table)\n",
    "print(\"Rows in staging:\", spark.table(stg_table).count())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_staging_prepare",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
