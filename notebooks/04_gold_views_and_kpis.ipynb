{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "418ee441-cec7-4192-a342-5978d93b7b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "pipeline = \"gold\"\n",
    "run_id = str(uuid.uuid4())\n",
    "\n",
    "silver_table = \"etl_demo.silver.customer_dim\"\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS etl_demo.gold\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS etl_demo.control\")\n",
    "\n",
    "# STARTED log (SQL-only to avoid type inference issues)\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO etl_demo.control.run_log\n",
    "SELECT\n",
    "  '{pipeline}' AS pipeline,\n",
    "  '{run_id}'   AS run_id,\n",
    "  CAST(NULL AS DATE) AS snapshot_date,\n",
    "  current_timestamp() AS started_at,\n",
    "  CAST(NULL AS TIMESTAMP) AS finished_at,\n",
    "  'STARTED' AS status,\n",
    "  CAST(NULL AS BIGINT) AS rows_written,\n",
    "  CAST(NULL AS STRING) AS message\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    # 1) current view\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW etl_demo.gold.customer_current AS\n",
    "    SELECT customer_id, name, city, email, valid_from\n",
    "    FROM {silver_table}\n",
    "    WHERE is_current = true\n",
    "    \"\"\")\n",
    "\n",
    "    # 2) latest date\n",
    "    s = spark.table(silver_table)\n",
    "    latest_date = s.select(F.max(\"valid_from\").alias(\"d\")).collect()[0][\"d\"]\n",
    "\n",
    "    # KPI table\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS etl_demo.gold.customer_daily_kpi (\n",
    "      kpi_date         DATE,\n",
    "      active_customers BIGINT,\n",
    "      new_customers    BIGINT,\n",
    "      changed_customers BIGINT,\n",
    "      closed_records   BIGINT,\n",
    "      run_time         TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    \"\"\")\n",
    "\n",
    "    active_customers = s.filter(F.col(\"is_current\") == True).count()\n",
    "\n",
    "    first_dates = s.groupBy(\"customer_id\").agg(F.min(\"valid_from\").alias(\"first_valid_from\"))\n",
    "    new_customers = first_dates.filter(F.col(\"first_valid_from\") == F.lit(latest_date)).count()\n",
    "\n",
    "    changed_customers = (\n",
    "        s.filter(F.col(\"valid_from\") == F.lit(latest_date))\n",
    "         .join(first_dates, on=\"customer_id\", how=\"inner\")\n",
    "         .filter(F.col(\"first_valid_from\") < F.lit(latest_date))\n",
    "         .select(\"customer_id\").distinct()\n",
    "         .count()\n",
    "    )\n",
    "\n",
    "    closed_records = s.filter(F.col(\"valid_to\") == F.lit(latest_date)).count()\n",
    "\n",
    "    kpi_df = spark.createDataFrame(\n",
    "        [(latest_date, active_customers, new_customers, changed_customers, closed_records)],\n",
    "        [\"kpi_date\", \"active_customers\", \"new_customers\", \"changed_customers\", \"closed_records\"]\n",
    "    ).withColumn(\"run_time\", F.current_timestamp())\n",
    "\n",
    "    kpi_df.createOrReplaceTempView(\"kpi_src\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "    MERGE INTO etl_demo.gold.customer_daily_kpi AS tgt\n",
    "    USING kpi_src AS src\n",
    "    ON tgt.kpi_date = src.kpi_date\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "    # SUCCESS update\n",
    "    spark.sql(f\"\"\"\n",
    "    UPDATE etl_demo.control.run_log\n",
    "    SET\n",
    "      snapshot_date = DATE('{latest_date}'),\n",
    "      finished_at = current_timestamp(),\n",
    "      status = 'SUCCESS',\n",
    "      rows_written = 1,\n",
    "      message = 'gold view + kpi upsert ok'\n",
    "    WHERE pipeline = '{pipeline}' AND run_id = '{run_id}'\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"âœ… GOLD done. Latest date:\", latest_date)\n",
    "\n",
    "except Exception as e:\n",
    "    err = str(e).replace(\"'\", \"''\")[:1500]\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    UPDATE etl_demo.control.run_log\n",
    "    SET\n",
    "      finished_at = current_timestamp(),\n",
    "      status = 'FAILED',\n",
    "      rows_written = 0,\n",
    "      message = '{err}'\n",
    "    WHERE pipeline = '{pipeline}' AND run_id = '{run_id}'\n",
    "    \"\"\")\n",
    "\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_gold_views_and_kpis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
